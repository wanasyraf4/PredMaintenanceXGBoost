{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics as st\n",
    "import optuna\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.feature_selection import f_regression\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.stats import weibull_min\n",
    "from sklearn import linear_model\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"Trend01_20221101.csv\")\n",
    "df2 = pd.read_csv(\"Trend01_20221102.csv\")\n",
    "df3 = pd.read_csv(\"Trend01_20221103.csv\")\n",
    "df4 = pd.read_csv(\"Trend01_20221104.csv\")\n",
    "df5 = pd.read_csv(\"Trend01_20221105.csv\")\n",
    "df6 = pd.read_csv(\"Trend01_20221106.csv\")\n",
    "df7 = pd.read_csv(\"Trend01_20221107.csv\")\n",
    "\n",
    "df8  = pd.read_csv(\"Trend01_20221108.csv\")\n",
    "df9  = pd.read_csv(\"Trend01_20221109.csv\")\n",
    "df10 = pd.read_csv(\"Trend01_20221110.csv\")\n",
    "df11 = pd.read_csv(\"Trend01_20221111.csv\")\n",
    "df12 = pd.read_csv(\"Trend01_20221112.csv\")\n",
    "df13 = pd.read_csv(\"Trend01_20221113.csv\")\n",
    "df14 = pd.read_csv(\"Trend01_20221114.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2, df3, df4, df5, df6, df7,\n",
    "                df8, df9, df10, df11, df12, df13, df14])\n",
    "\n",
    "# df.plot()\n",
    "time = df['F01'].to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "Time_A = df11['F01'].to_numpy()\n",
    "Time_proj = df12[\"F01\"].to_numpy  # TimeProj(t) = t + 1\n",
    "\n",
    "TCUI2_temp_A = cp.array(df11[\"PVI_T2031\"])\n",
    "\n",
    "torque_EX1_A = df11[\"PVI_T2097\"]\n",
    "torque_EX2_A = df11[\"PVI_T2052\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[189.72333 190.50824 207.22836 ... 226.11023 223.53198 227.52663]\n",
      "[190.8 190.8 190.8 ... 188.4 188.4 188.4]\n",
      "RMSE: 0.053154066459805416\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=0)\n",
    "\n",
    "features = ['PVI_T2097', 'PVI_T2052', 'PVI_T2031']\n",
    "target = 'PVI_T2031'\n",
    "\n",
    "dtrain = xgb.DMatrix(train_df[features], label=train_df[target])\n",
    "dtest = xgb.DMatrix(test_df[features], label=test_df[target])\n",
    "\n",
    "\n",
    "# Set up the XGBoost parameters\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'max_depth': 5,\n",
    "    'eta': 0.1,\n",
    "    'eval_metric': 'rmse'\n",
    "}\n",
    "\n",
    "# Train the XGBoost model\n",
    "model = xgb.train(params, dtrain, num_boost_round=100)\n",
    "\n",
    "\n",
    "#Predict on 20% test data\n",
    "predictions = model.predict(dtest)\n",
    "\n",
    "print(predictions)\n",
    "print(df['PVI_T2031'].to_numpy())\n",
    "\n",
    "predictions = model.predict(dtest)\n",
    "actuals = test_df[target].to_numpy()\n",
    "rmse = np.sqrt(np.mean((predictions - actuals)**2))\n",
    "print(\"RMSE:\", rmse)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241904\n"
     ]
    }
   ],
   "source": [
    "dtarget = xgb.DMatrix(df[features], label=df[target])\n",
    "prediction_new = model.predict(dtarget)\n",
    "\n",
    "print(len(prediction_new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48381\n",
      "241904\n"
     ]
    }
   ],
   "source": [
    "print(len(predictions))\n",
    "print(len(df['PVI_T2031'].to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetX =  np.array(df['PVI_T2031'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(x= time, y = prediction_new)\n",
    "fig.add_scatter(x = time, y = targetX, mode = 'lines')\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "':' expected after dictionary key (1882442527.py, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [96], line 15\u001b[1;36m\u001b[0m\n\u001b[1;33m    ...\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m ':' expected after dictionary key\n"
     ]
    }
   ],
   "source": [
    "#Time future projection\n",
    "\n",
    "delta = pd.to_datetime(df[\"F01\"])\n",
    "last_time = df['F01'].max()\n",
    "\n",
    "\n",
    "# Calculate the next day's time\n",
    "next_day = last_time + timedelta(days=1)\n",
    "\n",
    "# Create a new row for the next day's data\n",
    "new_row = pd.DataFrame({'time': [next_day],\n",
    "                        'sensor_1': [0],\n",
    "                        'sensor_2': [0],\n",
    "                        'sensor_3': [0],\n",
    "                        # ...\n",
    "                       })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Define the objective function for Optuna to minimize\n",
    "def objective(trial):\n",
    "    # Set the XGBoost hyperparameters for this trial\n",
    "    params = {\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.01, 1.0),\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 200),\n",
    "        \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-8, 1.0),\n",
    "        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
    "    }\n",
    "    \n",
    "    # Train an XGBoost model using the set of hyperparameters\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Return the negative accuracy so that Optuna minimizes this value\n",
    "    return -accuracy\n",
    "\n",
    "# Create an Optuna study\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "\n",
    "# Run the optimization\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df1, df2, df3, df4, df5, df6, df7,\n",
    "                df8, df9, df10, df11, df12, df13, df14])\n",
    "\n",
    "time = df['F01'].to_numpy()\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=0)\n",
    "\n",
    "features = ['PVI_T2097', 'PVI_T2052', 'PVI_T2031']\n",
    "target = 'PVI_T2031'\n",
    "\n",
    "dtrain = xgb.DMatrix(train_df[features], label=train_df[target])\n",
    "dtest = xgb.DMatrix(test_df[features], label=test_df[target])\n",
    "\n",
    "#optuna optimisation loop\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 8),\n",
    "        'eta': trial.suggest_uniform('eta', 0.05, 0.3),\n",
    "    }\n",
    "\n",
    "    #xgboost train\n",
    "    model = xgb.train(params, dtrain, num_boost_round=100)\n",
    "    predictions = model.predict(dtest)\n",
    "    actuals = test_df[target].to_numpy()\n",
    "    rmse = np.sqrt(np.mean((predictions - actuals)**2))\n",
    "    return rmse\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "print('Best hyperparameters:', study.best_params)\n",
    "print('Best RMSE:', study.best_value)\n",
    "\n",
    "\n",
    "# Use the best hyperparameters from Optuna to train a final model on all the training data\n",
    "best_params = study.best_params\n",
    "best_model = xgb.train(best_params, xgb.DMatrix(df[features], label=df[target]), num_boost_round=100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = xgb.DMatrix(df[features], label=df[target])\n",
    "\n",
    "# Use the final model for prediction on new data\n",
    "new_data = dat # data that need to make predictions on\n",
    "\n",
    "# new_dmatrix = xgb.DMatrix(new_data)\n",
    "new_predictions = best_model.predict(dat)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 (main, Oct 24 2022, 18:26:48) [MSC v.1933 64 bit (AMD64)]"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [
       "c#",
       "C#"
      ],
      "languageName": "C#",
      "name": "csharp"
     },
     {
      "aliases": [
       "frontend"
      ],
      "languageName": null,
      "name": "vscode"
     }
    ]
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "554f92ad98831055254315a845b84ad857909171ef8b6f1bb29bea14ee500f5b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
